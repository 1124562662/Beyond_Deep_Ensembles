{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "DATA_PATH = \"../data\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from src.log_mock import PrintLog\n",
    "log = PrintLog()\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wapi = wandb.Api()\n",
    "runs = wapi.runs(\"bayes/fmow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    print(run.name, run.summary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import dateutil\n",
    "import datetime\n",
    "\n",
    "def create_plot_data_for_run(run):\n",
    "    parts = run.name.split(\"-\")\n",
    "    if len(parts) > 2:\n",
    "        model_name = parts[0] + \"-\" + parts[1]\n",
    "    else:\n",
    "        model_name = parts[0]\n",
    "\n",
    "    worst_acc = 1\n",
    "    worst_acc_group = \"None\"\n",
    "    for name, results in run.summary[\"test_results\"].items():\n",
    "        if \"region\" in name and name != \"worst_region_acc\":\n",
    "            if results[\"accuracy\"] < worst_acc:\n",
    "                worst_acc = results[\"accuracy\"]\n",
    "                worst_acc_group = name\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"worst_region_acc\": run.summary[\"test_results\"][\"worst_region_acc\"],\n",
    "        \"all accuracy\": run.summary[\"test_results\"][\"all\"][\"accuracy\"],\n",
    "        \"all log likelihood\": run.summary[\"test_results\"][\"all\"][\"log_likelihood\"],\n",
    "        \"all ece\": run.summary[\"test_results\"][\"all\"][\"ece\"],\n",
    "        \"all sece\": run.summary[\"test_results\"][\"all\"][\"sece\"],\n",
    "        \"worst_acc accuracy\": run.summary[\"test_results\"][worst_acc_group][\"accuracy\"],\n",
    "        \"worst_acc sece\": run.summary[\"test_results\"][worst_acc_group][\"sece\"],\n",
    "        \"worst_acc ece\": run.summary[\"test_results\"][worst_acc_group][\"ece\"],\n",
    "        \"worst_acc log_likelihood\": run.summary[\"test_results\"][worst_acc_group][\"log_likelihood\"]\n",
    "    }\n",
    "\n",
    "def plot(data, value):\n",
    "    plot = px.box(data, x=\"model\", y=value, color=\"model\")\n",
    "    return plot\n",
    "\n",
    "def pareto_plot(data, x, y):\n",
    "    plot = px.scatter(data, x=x, error_x=f\"{x}_std\", y=y, error_y=f\"{y}_std\", color=\"model\")\n",
    "    return plot\n",
    "\n",
    "def build_data(runs):\n",
    "    rows = []\n",
    "    for run in runs:\n",
    "        if dateutil.parser.parse(run.created_at) < datetime.datetime(2023, 3, 10, 10, 0):\n",
    "            continue\n",
    "        if run.state != \"finished\":\n",
    "            continue\n",
    "        if \"old\" in run.tags:\n",
    "            print(\"Skipping old run \" + run.name)\n",
    "            continue\n",
    "        if \"test_results\" not in run.summary:\n",
    "            print(\"Skipping crashed run \" + run.name)\n",
    "            continue\n",
    "        rows.append(create_plot_data_for_run(run))\n",
    "    return pd.DataFrame.from_dict(rows)\n",
    "\n",
    "def aggregate_data(data):\n",
    "    aggregated_data = data.groupby([\"model\"]).agg({\n",
    "        \"model\": \"first\",\n",
    "        \"worst_region_acc\": [\"mean\", \"sem\"],\n",
    "        \"all accuracy\": [\"mean\", \"sem\"],\n",
    "        \"all log likelihood\": [\"mean\", \"sem\"], \n",
    "        \"all sece\": [\"mean\", \"sem\"],\n",
    "        \"all ece\": [\"mean\", \"sem\"],\n",
    "        \"worst_acc accuracy\": [\"mean\", \"sem\"],\n",
    "        \"worst_acc sece\": [\"mean\", \"sem\"],\n",
    "        \"worst_acc ece\": [\"mean\", \"sem\"],\n",
    "        \"worst_acc log_likelihood\": [\"mean\", \"sem\"],\n",
    "    })\n",
    "    aggregated_data.columns = [a[0] + \"_std\" if a[1] == \"sem\" else a[0] for a in aggregated_data.columns.to_flat_index()]\n",
    "    aggregated_data[\"worst_region_acc_std\"] *= 2.0\n",
    "    aggregated_data[\"all accuracy_std\"] *= 2.0\n",
    "    aggregated_data[\"all log likelihood_std\"] *= 2.0\n",
    "    aggregated_data[\"all sece_std\"] *= 2.0\n",
    "    aggregated_data[\"all ece_std\"] *= 2.0\n",
    "    aggregated_data[\"worst_acc accuracy_std\"] *= 2.0\n",
    "    aggregated_data[\"worst_acc sece_std\"] *= 2.0\n",
    "    aggregated_data[\"worst_acc ece_std\"] *= 2.0\n",
    "    aggregated_data[\"worst_acc log_likelihood_std\"] *= 2.0\n",
    "    return aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aggregate_data(build_data(runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_plot(data, \"worst_acc accuracy\", \"worst_acc sece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_plot(data, \"all accuracy\", \"all sece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_names = [\n",
    "    (\"map-1\", \"MAP\"),\n",
    "    (\"map-5\", \"Deep Ensemble\"),\n",
    "    (\"mcd_p0.1-1\", \"MCD\"),\n",
    "    (\"mcd-5\", \"MultiMCD\"),\n",
    "    (\"swag-1\", \"SWAG\"),\n",
    "    (\"swag-5\", \"MultiSWAG\"),\n",
    "    (\"swag_ll-1\", \"LL SWAG\"),\n",
    "    (\"laplace-1\", \"LL Laplace\"),\n",
    "    (\"laplace-5\", \"LL MultiLaplace\"),\n",
    "    (\"bbb-1\", \"LL BBB\"),\n",
    "    (\"bbb-5\", \"LL MultiBBB\"),\n",
    "    (\"rank1-1\", \"Rank-1 VI\"),\n",
    "    (\"ll_ivon-1\", \"LL iVON\"),\n",
    "    (\"ll_ivon-5\", \"LL MultiiVON\"),\n",
    "    (\"svgd-1\", \"SVGD\"),\n",
    "    (\"sngp\", \"SNGP\"),\n",
    "]\n",
    "\n",
    "def num(value, std, best=None, ty=None):\n",
    "    value = float(value)\n",
    "    std = float(std)\n",
    "    num_string = f\"{value:.3f} \\\\pm {std:.3f}\"\n",
    "\n",
    "    if best is None or ty is None:\n",
    "        return f\"${num_string}$\"\n",
    "\n",
    "    if ty == \"max\":\n",
    "        if value >= best:\n",
    "            num_string = f\"\\\\bm{{{num_string}}}\"\n",
    "    elif ty == \"min\":\n",
    "        if value <= best:\n",
    "            num_string = f\"\\\\bm{{{num_string}}}\"\n",
    "    elif ty == \"zero\":\n",
    "        if abs(value) <= best:\n",
    "            num_string = f\"\\\\bm{{{num_string}}}\"\n",
    "    return f\"${num_string}$\"\n",
    "\n",
    "def col_name(name, align):\n",
    "    return f\"\\\\multicolumn{{1}}{{{align}}}{{{name}}}\"\n",
    "\n",
    "def create_table(data, prefix):\n",
    "    print(\"\\\\begin{tabular}{l|rrrrrr}\")\n",
    "    print(f\"    {col_name('Model', 'l')} & {col_name('WR Accuracy', 'c')} & {col_name('WR ECE', 'c')} & {col_name('WR sECE', 'c')} & {col_name('Avg Accuracy', 'c')} & {col_name('Avg ECE', 'c')} & {col_name('Avg sECE', 'c')} \\\\\\\\\")\n",
    "    print(\"    \\\\hline\")\n",
    "\n",
    "    best_acc, best_acc_std = 0, 0\n",
    "    best_ece, best_ece_std = 1000, 0\n",
    "    best_sece, best_sece_std = 1000, 0\n",
    "    best_avg_acc, best_avg_acc_std = 0, 0\n",
    "    best_avg_ece, best_avg_ece_std = 1000, 0\n",
    "    best_avg_sece, best_avg_sece_std = 1000, 0\n",
    "\n",
    "    for algo, name in algo_names:\n",
    "        row = data[data[\"model\"] == algo]\n",
    "\n",
    "        if float(row[prefix + \"worst_acc accuracy\"]) > best_acc:\n",
    "            best_acc = float(row[prefix + \"worst_acc accuracy\"])\n",
    "            best_acc_std = float(row[prefix + \"worst_acc accuracy_std\"])\n",
    "        \n",
    "        if float(row[prefix + \"worst_acc ece\"]) < best_ece:\n",
    "            best_ece = float(row[prefix + \"worst_acc ece\"])\n",
    "            best_ece_std = float(row[prefix + \"worst_acc ece_std\"])\n",
    "        \n",
    "        if abs(float(row[prefix + \"worst_acc sece\"])) < best_sece:\n",
    "            best_sece = abs(float(row[prefix + \"worst_acc sece\"]))\n",
    "            best_sece_std = float(row[prefix + \"worst_acc sece_std\"])\n",
    "        \n",
    "        if float(row[prefix + \"all accuracy\"]) > best_avg_acc:\n",
    "            best_avg_acc = float(row[prefix + \"all accuracy\"])\n",
    "            best_avg_acc_std = float(row[prefix + \"all accuracy_std\"])\n",
    "        \n",
    "        if float(row[prefix + \"all ece\"]) < best_avg_ece:\n",
    "            best_avg_ece = float(row[prefix + \"all ece\"])\n",
    "            best_avg_ece_std = float(row[prefix + \"all ece_std\"])\n",
    "        \n",
    "        if abs(float(row[prefix + \"all sece\"])) < best_avg_sece:\n",
    "            best_avg_sece = abs(float(row[prefix + \"all sece\"]))\n",
    "            best_avg_sece_std = float(row[prefix + \"all sece_std\"])\n",
    "\n",
    "    best_acc -= best_acc_std\n",
    "    best_ece += best_ece_std\n",
    "    best_sece = abs(best_sece) + best_sece_std\n",
    "\n",
    "    best_avg_acc -= best_avg_acc_std\n",
    "    best_avg_ece += best_avg_ece_std\n",
    "    best_avg_sece = abs(best_avg_sece) + best_avg_sece_std\n",
    "\n",
    "    for algo, name in algo_names:\n",
    "        row = data[data[\"model\"] == algo]\n",
    "        print(f\"    {name} & {num(row[prefix + 'worst_acc accuracy'], row[prefix + 'worst_acc accuracy_std'], best_acc, 'max')} & {num(row[prefix + 'worst_acc ece'], row[prefix + 'worst_acc ece_std'], best_ece, 'min')} & {num(row[prefix + 'worst_acc sece'], row[prefix + 'worst_acc sece_std'], best_sece, 'zero')} & {num(row[prefix + 'all accuracy'], row[prefix + 'all accuracy_std'], best_avg_acc, 'max')} & {num(row[prefix + 'all ece'], row[prefix + 'all ece_std'], best_avg_ece, 'min')} & {num(row[prefix + 'all sece'], row[prefix + 'all sece_std'], best_avg_sece, 'zero')} \\\\\\\\\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "create_table(data, \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
